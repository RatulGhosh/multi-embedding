{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import tempfile\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the BSD-style license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "# LASER  Language-Agnostic SEntence Representations\n",
    "# is a toolkit to calculate multilingual sentence embeddings\n",
    "# and to use them for document classification, bitext filtering\n",
    "# and mining\n",
    "#\n",
    "# --------------------------------------------------------\n",
    "#\n",
    "# Helper functions for tokenization and BPE\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import numpy as np\n",
    "from subprocess import run, check_output, DEVNULL\n",
    "\n",
    "# get environment\n",
    "LASER = \"~/Desktop/multi-embedding/LASER-master\"\n",
    "\n",
    "\n",
    "FASTBPE = LASER + '/tools-external/fastBPE/fast'\n",
    "MOSES_BDIR = LASER + '/tools-external/moses-tokenizer/tokenizer/'\n",
    "MOSES_TOKENIZER = MOSES_BDIR + 'tokenizer.perl -q -no-escape -threads 20 -l '\n",
    "MOSES_LC = MOSES_BDIR + 'lowercase.perl'\n",
    "NORM_PUNC = MOSES_BDIR + 'normalize-punctuation.perl -l '\n",
    "DESCAPE = MOSES_BDIR + 'deescape-special-chars.perl'\n",
    "REM_NON_PRINT_CHAR = MOSES_BDIR + 'remove-non-printing-char.perl'\n",
    "\n",
    "# Romanization (Greek only)\n",
    "ROMAN_LC = 'python3 ' + LASER + '/source/lib/romanize_lc.py -l '\n",
    "\n",
    "# Mecab tokenizer for Japanese\n",
    "MECAB = LASER + '/tools-external/mecab'\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#\n",
    "# Tokenize a line\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "def TokenLine(line, lang='en', lower_case=True, romanize=False):\n",
    "    assert lower_case, 'lower case is needed by all the models'\n",
    "    roman = lang if romanize else 'none'\n",
    "    tok = check_output(\n",
    "            REM_NON_PRINT_CHAR\n",
    "            + '|' + NORM_PUNC + lang\n",
    "            + '|' + DESCAPE\n",
    "            + '|' + MOSES_TOKENIZER + lang\n",
    "            + ('| python3 -m jieba -d ' if lang == 'zh' else '')\n",
    "            + ('|' + MECAB + '/bin/mecab -O wakati -b 50000 ' if lang == 'ja' else '')\n",
    "            + '|' + ROMAN_LC + roman,\n",
    "            input=line,\n",
    "            encoding='UTF-8',\n",
    "            shell=True)\n",
    "    return tok.strip()\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#\n",
    "# Tokenize a file\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "def Token(inp_fname, out_fname, lang='en',\n",
    "          lower_case=True, romanize=False, descape=False,\n",
    "          verbose=False, over_write=False, gzip=False):\n",
    "    assert lower_case, 'lower case is needed by all the models'\n",
    "    assert not over_write, 'over-write is not yet implemented'\n",
    "    if not os.path.isfile(out_fname):\n",
    "        cat = 'zcat ' if gzip else 'cat '\n",
    "        roman = lang if romanize else 'none'\n",
    "        # handle some iso3 langauge codes\n",
    "        if lang in ('cmn', 'wuu', 'yue'):\n",
    "            lang = 'zh'\n",
    "        if lang in ('jpn'):\n",
    "            lang = 'ja'\n",
    "        if verbose:\n",
    "            print(' - Tokenizer: {} in language {} {} {}'\n",
    "                  .format(os.path.basename(inp_fname), lang,\n",
    "                          '(gzip)' if gzip else '',\n",
    "                          '(de-escaped)' if descape else '',\n",
    "                          '(romanized)' if romanize else ''))\n",
    "        run(cat + inp_fname\n",
    "            + '|' + REM_NON_PRINT_CHAR\n",
    "            + '|' + NORM_PUNC + lang\n",
    "            + ('|' + DESCAPE if descape else '')\n",
    "            + '|' + MOSES_TOKENIZER + lang\n",
    "            + ('| python3 -m jieba -d ' if lang == 'zh' else '')\n",
    "            + ('|' + MECAB + '/bin/mecab -O wakati -b 50000 ' if lang == 'ja' else '')\n",
    "            + '|' + ROMAN_LC + roman\n",
    "            + '>' + out_fname,\n",
    "            env=dict(os.environ, LD_LIBRARY_PATH=MECAB + '/lib'),\n",
    "            shell=True)\n",
    "    elif not over_write and verbose:\n",
    "        print(' - Tokenizer: {} exists already'\n",
    "              .format(os.path.basename(out_fname), lang))\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#\n",
    "# Apply FastBPE for one line\n",
    "# This implementation is highly suboptimal since we have to spawn a new\n",
    "# process and load the BPE codes for each line !!\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "def BPEfastApplyLine(line, bpe_codes):\n",
    "    bpe_vocab = bpe_codes.replace('fcodes', 'fvocab')\n",
    "    if not os.path.isfile(bpe_vocab):\n",
    "        print(' - fast BPE: focab file not found {}'.format(bpe_vocab))\n",
    "        bpe_vocab = ''\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        ifn = os.path.join(tmpdir, 'tok')\n",
    "        ofn = os.path.join(tmpdir, 'bpe')\n",
    "        with open(ifn, 'w') as f:\n",
    "            f.write('{}\\n'.format(line))\n",
    "        run(FASTBPE + ' applybpe ' + ofn + ' ' + ifn\n",
    "            + ' ' + bpe_codes + ' ' + bpe_vocab,\n",
    "            shell=True, stderr=DEVNULL)\n",
    "        with open(ofn, 'r') as f:\n",
    "            bpe = f.readlines()\n",
    "        assert len(bpe) == 1, 'ERROR: unexpected BPE output'\n",
    "        return bpe[0]\n",
    "\n",
    "\n",
    "def BPEfastApply(inp_fname, out_fname, bpe_codes,\n",
    "                 verbose=False, over_write=False):\n",
    "    if not os.path.isfile(out_fname):\n",
    "        if verbose:\n",
    "            print(' - fast BPE: processing {}'\n",
    "                  .format(os.path.basename(inp_fname)))\n",
    "        bpe_vocab = bpe_codes.replace('fcodes', 'fvocab')\n",
    "        if not os.path.isfile(bpe_vocab):\n",
    "            print(' - fast BPE: focab file not found {}'.format(bpe_vocab))\n",
    "            bpe_vocab = ''\n",
    "        run(FASTBPE + ' applybpe '\n",
    "            + out_fname + ' ' + inp_fname\n",
    "            + ' ' + bpe_codes\n",
    "            + ' ' + bpe_vocab, shell=True, stderr=DEVNULL)\n",
    "    elif not over_write and verbose:\n",
    "        print(' - fast BPE: {} exists already'\n",
    "              .format(os.path.basename(out_fname)))\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#\n",
    "# Split long lines into multiple sentences at \".\"\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "def SplitLines(ifname, of_txt, of_sid):\n",
    "    if os.path.isfile(of_txt):\n",
    "        print(' - SplitLines: {} already exists'.format(of_txt))\n",
    "        return\n",
    "    nl = 0\n",
    "    nl_sp = 0\n",
    "    maxw = 0\n",
    "    maxw_sp = 0\n",
    "    fp_sid = open(of_sid, 'w')\n",
    "    fp_txt = open(of_txt, 'w')\n",
    "    with open(ifname, 'r') as ifp:\n",
    "        for line in ifp:\n",
    "            print('{:d}'.format(nl), file=fp_sid)  # store current sentence ID\n",
    "            nw = 0\n",
    "            words = line.strip().split()\n",
    "            maxw = max(maxw, len(words))\n",
    "            for i, word in enumerate(words):\n",
    "                if word == '.' and i != len(words)-1:\n",
    "                    if nw > 0:\n",
    "                        print(' {}'.format(word), file=fp_txt)\n",
    "                    else:\n",
    "                        print('{}'.format(word), file=fp_txt)\n",
    "                    # store current sentence ID\n",
    "                    print('{:d}'.format(nl), file=fp_sid)\n",
    "                    nl_sp += 1\n",
    "                    maxw_sp = max(maxw_sp, nw+1)\n",
    "                    nw = 0\n",
    "                else:\n",
    "                    if nw > 0:\n",
    "                        print(' {}'.format(word), end='', file=fp_txt)\n",
    "                    else:\n",
    "                        print('{}'.format(word), end='', file=fp_txt)\n",
    "                    nw += 1\n",
    "            if nw > 0:\n",
    "                # handle remainder of sentence\n",
    "                print('', file=fp_txt)\n",
    "                nl_sp += 1\n",
    "                maxw_sp = max(maxw_sp, nw+1)\n",
    "            nl += 1\n",
    "    print(' - Split sentences: {}'.format(ifname))\n",
    "    print(' -                  lines/max words: {:d}/{:d} -> {:d}/{:d}'\n",
    "          .format(nl, maxw, nl_sp, maxw_sp))\n",
    "    fp_sid.close()\n",
    "    fp_txt.close()\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#\n",
    "# Join embeddings of previously split lines (average)\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "def JoinEmbed(if_embed, sid_fname, of_embed, dim=1024):\n",
    "    if os.path.isfile(of_embed):\n",
    "        print(' - JoinEmbed: {} already exists'.format(of_embed))\n",
    "        return\n",
    "    # read the input embeddings\n",
    "    em_in = np.fromfile(if_embed, dtype=np.float32, count=-1).reshape(-1, dim)\n",
    "    ninp = em_in.shape[0]\n",
    "    print(' - Combine embeddings:')\n",
    "    print('                input: {:s} {:d} sentences'.format(if_embed, ninp))\n",
    "\n",
    "    # get all sentence IDs\n",
    "    sid = np.empty(ninp, dtype=np.int32)\n",
    "    i = 0\n",
    "    with open(sid_fname, 'r') as fp_sid:\n",
    "        for line in fp_sid:\n",
    "            sid[i] = int(line)\n",
    "            i += 1\n",
    "    nout = sid.max() + 1\n",
    "    print('                IDs: {:s}, {:d} sentences'.format(sid_fname, nout))\n",
    "\n",
    "    # combining\n",
    "    em_out = np.zeros((nout, dim), dtype=np.float32)\n",
    "    cnt = np.zeros(nout, dtype=np.int32)\n",
    "    for i in range(ninp):\n",
    "        idx = sid[i]\n",
    "        em_out[idx] += em_in[i]  # cumulate sentence vectors\n",
    "        cnt[idx] += 1\n",
    "\n",
    "    if (cnt == 0).astype(int).sum() > 0:\n",
    "        print('ERROR: missing lines')\n",
    "        sys.exit(1)\n",
    "\n",
    "    # normalize\n",
    "    for i in range(nout):\n",
    "        em_out[i] /= cnt[i]\n",
    "\n",
    "    print('                output: {:s}'.format(of_embed))\n",
    "    em_out.tofile(of_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACE_NORMALIZER = re.compile(\"\\s+\")\n",
    "Batch = namedtuple('Batch', 'srcs tokens lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buffered_read(fp, buffer_size):\n",
    "    buffer = []\n",
    "    for src_str in fp:\n",
    "        buffer.append(src_str.strip())\n",
    "        if len(buffer) >= buffer_size:\n",
    "            yield buffer\n",
    "            buffer = []\n",
    "\n",
    "    if len(buffer) > 0:\n",
    "        yield buffer\n",
    "\n",
    "\n",
    "def buffered_arange(max):\n",
    "    if not hasattr(buffered_arange, 'buf'):\n",
    "        buffered_arange.buf = torch.LongTensor()\n",
    "    if max > buffered_arange.buf.numel():\n",
    "        torch.arange(max, out=buffered_arange.buf)\n",
    "    return buffered_arange.buf[:max]\n",
    "\n",
    "\n",
    "# TODO Do proper padding from the beginning\n",
    "def convert_padding_direction(src_tokens, padding_idx, right_to_left=False, left_to_right=False):\n",
    "    assert right_to_left ^ left_to_right\n",
    "    pad_mask = src_tokens.eq(padding_idx)\n",
    "    if not pad_mask.any():\n",
    "        # no padding, return early\n",
    "        return src_tokens\n",
    "    if left_to_right and not pad_mask[:, 0].any():\n",
    "        # already right padded\n",
    "        return src_tokens\n",
    "    if right_to_left and not pad_mask[:, -1].any():\n",
    "        # already left padded\n",
    "        return src_tokens\n",
    "    max_len = src_tokens.size(1)\n",
    "    range = buffered_arange(max_len).type_as(src_tokens).expand_as(src_tokens)\n",
    "    num_pads = pad_mask.long().sum(dim=1, keepdim=True)\n",
    "    if right_to_left:\n",
    "        index = torch.remainder(range - num_pads, max_len)\n",
    "    else:\n",
    "        index = torch.remainder(range + num_pads, max_len)\n",
    "    return src_tokens.gather(1, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEncoder:\n",
    "\n",
    "    def __init__(self, model_path, max_sentences=None, max_tokens=None, cpu=False, fp16=False, sort_kind='quicksort'):\n",
    "        self.use_cuda = torch.cuda.is_available() and not cpu\n",
    "        self.max_sentences = max_sentences\n",
    "        self.max_tokens = max_tokens\n",
    "        if self.max_tokens is None and self.max_sentences is None:\n",
    "            self.max_sentences = 1\n",
    "\n",
    "        state_dict = torch.load(model_path)\n",
    "        self.encoder = Encoder(**state_dict['params'])\n",
    "        self.encoder.load_state_dict(state_dict['model'])\n",
    "        self.dictionary = state_dict['dictionary']\n",
    "        self.pad_index = self.dictionary['<pad>']\n",
    "        self.eos_index = self.dictionary['</s>']\n",
    "        self.unk_index = self.dictionary['<unk>']\n",
    "        if fp16:\n",
    "            self.encoder.half()\n",
    "        if self.use_cuda:\n",
    "            print(' - transfer encoder to GPU')\n",
    "            self.encoder.cuda()\n",
    "        self.sort_kind = sort_kind\n",
    "\n",
    "    def _process_batch(self, batch):\n",
    "        tokens = batch.tokens\n",
    "        lengths = batch.lengths\n",
    "        if self.use_cuda:\n",
    "            tokens = tokens.cuda()\n",
    "            lengths = lengths.cuda()\n",
    "        self.encoder.eval()\n",
    "        embeddings = self.encoder(tokens, lengths)['sentemb']\n",
    "        return embeddings.detach().cpu().numpy()\n",
    "\n",
    "    def _tokenize(self, line):\n",
    "        tokens = SPACE_NORMALIZER.sub(\" \", line).strip().split()\n",
    "        ntokens = len(tokens)\n",
    "        ids = torch.LongTensor(ntokens + 1)\n",
    "        for i, token in enumerate(tokens):\n",
    "            ids[i] = self.dictionary.get(token, self.unk_index)\n",
    "        ids[ntokens] = self.eos_index\n",
    "        return ids\n",
    "\n",
    "    def _make_batches(self, lines):\n",
    "        tokens = [self._tokenize(line) for line in lines]\n",
    "        lengths = np.array([t.numel() for t in tokens])\n",
    "        indices = np.argsort(-lengths, kind=self.sort_kind)\n",
    "\n",
    "        def batch(tokens, lengths, indices):\n",
    "            toks = tokens[0].new_full((len(tokens), tokens[0].shape[0]), self.pad_index)\n",
    "            for i in range(len(tokens)):\n",
    "                toks[i, -tokens[i].shape[0]:] = tokens[i]\n",
    "            return Batch(\n",
    "                srcs=None,\n",
    "                tokens=toks,\n",
    "                lengths=torch.LongTensor(lengths)\n",
    "            ), indices\n",
    "\n",
    "        batch_tokens, batch_lengths, batch_indices = [], [], []\n",
    "        ntokens = nsentences = 0\n",
    "        for i in indices:\n",
    "            if nsentences > 0 and ((self.max_tokens is not None and ntokens + lengths[i] > self.max_tokens) or \n",
    "                                   (self.max_sentences is not None and nsentences == self.max_sentences)):\n",
    "                yield batch(batch_tokens, batch_lengths, batch_indices)\n",
    "                ntokens = nsentences = 0\n",
    "                batch_tokens, batch_lengths, batch_indices = [], [], []\n",
    "            batch_tokens.append(tokens[i])\n",
    "            batch_lengths.append(lengths[i])\n",
    "            batch_indices.append(i)\n",
    "            ntokens += tokens[i].shape[0]\n",
    "            nsentences += 1\n",
    "        if nsentences > 0:\n",
    "            yield batch(batch_tokens, batch_lengths, batch_indices)\n",
    "\n",
    "    def encode_sentences(self, sentences):\n",
    "        indices = []\n",
    "        results = []\n",
    "        for batch, batch_indices in self._make_batches(sentences):\n",
    "            indices.extend(batch_indices)\n",
    "            results.append(self._process_batch(batch))\n",
    "        return np.vstack(results)[np.argsort(indices, kind=self.sort_kind)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_embeddings, padding_idx, embed_dim=320, hidden_size=512, num_layers=1, bidirectional=False,\n",
    "        left_pad=True, padding_value=0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.padding_idx = padding_idx\n",
    "        self.embed_tokens = nn.Embedding(num_embeddings, embed_dim, padding_idx=self.padding_idx)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        self.left_pad = left_pad\n",
    "        self.padding_value = padding_value\n",
    "\n",
    "        self.output_units = hidden_size\n",
    "        if bidirectional:\n",
    "            self.output_units *= 2\n",
    "\n",
    "    def forward(self, src_tokens, src_lengths):\n",
    "        if self.left_pad:\n",
    "            # convert left-padding to right-padding\n",
    "            src_tokens = convert_padding_direction(\n",
    "                src_tokens,\n",
    "                self.padding_idx,\n",
    "                left_to_right=True,\n",
    "            )\n",
    "\n",
    "        bsz, seqlen = src_tokens.size()\n",
    "\n",
    "        # embed tokens\n",
    "        x = self.embed_tokens(src_tokens)\n",
    "\n",
    "        # B x T x C -> T x B x C\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        # pack embedded source tokens into a PackedSequence\n",
    "        packed_x = nn.utils.rnn.pack_padded_sequence(x, src_lengths.data.tolist())\n",
    "\n",
    "        # apply LSTM\n",
    "        if self.bidirectional:\n",
    "            state_size = 2 * self.num_layers, bsz, self.hidden_size\n",
    "        else:\n",
    "            state_size = self.num_layers, bsz, self.hidden_size\n",
    "        h0 = x.data.new(*state_size).zero_()\n",
    "        c0 = x.data.new(*state_size).zero_()\n",
    "        packed_outs, (final_hiddens, final_cells) = self.lstm(packed_x, (h0, c0))\n",
    "\n",
    "        # unpack outputs and apply dropout\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(packed_outs, padding_value=self.padding_value)\n",
    "        assert list(x.size()) == [seqlen, bsz, self.output_units]\n",
    "\n",
    "        if self.bidirectional:\n",
    "\n",
    "            def combine_bidir(outs):\n",
    "                return torch.cat([\n",
    "                    torch.cat([outs[2 * i], outs[2 * i + 1]], dim=0).view(1, bsz, self.output_units)\n",
    "                    for i in range(self.num_layers)\n",
    "                ], dim=0)\n",
    "\n",
    "            final_hiddens = combine_bidir(final_hiddens)\n",
    "            final_cells = combine_bidir(final_cells)\n",
    "\n",
    "        encoder_padding_mask = src_tokens.eq(self.padding_idx).t()\n",
    "\n",
    "        # Set padded outputs to -inf so they are not selected by max-pooling\n",
    "        padding_mask = src_tokens.eq(self.padding_idx).t().unsqueeze(-1)\n",
    "        if padding_mask.any():\n",
    "            x = x.float().masked_fill_(padding_mask, float('-inf')).type_as(x)\n",
    "\n",
    "        # Build the sentence embedding by max-pooling over the encoder outputs\n",
    "        sentemb = x.max(dim=0)[0]\n",
    "\n",
    "        return {\n",
    "            'sentemb': sentemb,\n",
    "            'encoder_out': (x, final_hiddens, final_cells),\n",
    "            'encoder_padding_mask': encoder_padding_mask if encoder_padding_mask.any() else None\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - transfer encoder to GPU\n"
     ]
    }
   ],
   "source": [
    "encoder = SentenceEncoder(\"LASER-master/models/bilstm.eparl21.2018-11-19.pt\",\n",
    "                          max_sentences=None,\n",
    "                          max_tokens=12000,\n",
    "                          sort_kind='mergesort',\n",
    "                          cpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EncodeFilep(encoder, inp_file, buffer_size=10000, verbose=True):\n",
    "    embeddings = []\n",
    "    n = 0\n",
    "    t = time.time()\n",
    "    for sentences in tqdm(buffered_read(inp_file, buffer_size)):\n",
    "        embeddings.extend(encoder.encode_sentences(sentences))\n",
    "        n += len(sentences)\n",
    "        if verbose and n % 10000 == 0:\n",
    "            print('\\r - Encoder: {:d} sentences'.format(n), end='')\n",
    "    if verbose:\n",
    "        print('\\r - Encoder: {:d} sentences'.format(n), end='')\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EncodeFile(encoder, inp_fname,\n",
    "              buffer_size=10000, verbose=False, over_write=False,\n",
    "              inp_encoding='utf-8'):\n",
    "    # TODO :handle over write\n",
    "    fin = open(inp_fname, 'r', encoding=inp_encoding, errors='surrogateescape') if len(inp_fname) > 0 else sys.stdin\n",
    "    embeddings = EncodeFilep(encoder, fin, buffer_size=buffer_size, verbose=verbose)\n",
    "    fin.close()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(token_lang, file_name, text_list):\n",
    "    token_lang = token_lang\n",
    "    bpe_codes = 'LASER-master/models/eparl21.fcodes'\n",
    "\n",
    "    with open(file_name, 'w') as f:\n",
    "        for item in text_list:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        ifname =  file_name# stdin will be used\n",
    "        if token_lang != '--':\n",
    "            tok_fname = os.path.join('', 'tok')\n",
    "            Token(ifname,\n",
    "                  tok_fname,\n",
    "                  lang=token_lang,\n",
    "                  romanize=True if token_lang == 'el' else False,\n",
    "                  lower_case=True, gzip=False,\n",
    "                  verbose=True, over_write=False)\n",
    "            ifname = tok_fname\n",
    "\n",
    "        if bpe_codes:\n",
    "            bpe_fname = os.path.join('', 'bpe')\n",
    "            BPEfastApply(ifname,\n",
    "                         bpe_fname,\n",
    "                         bpe_codes,\n",
    "                         verbose=True, over_write=False)\n",
    "            ifname = bpe_fname    \n",
    "\n",
    "        embedings = EncodeFile(encoder, ifname)\n",
    "        os.remove(\"tok\")\n",
    "        os.remove(\"bpe\")\n",
    "        return embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 59.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Tokenizer: tok exists already\n",
      " - fast BPE: bpe exists already\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedings_eco = get_embeddings('en', 'raw_file_eco.txt', sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1024)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedings_eco.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "661511"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmslib\n",
    "\n",
    "def create_nmslib_search_index(numpy_vectors):\n",
    "    \"\"\"Create search index using nmslib.\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    numpy_vectors : numpy.array\n",
    "        The matrix of vectors\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "    nmslib object that has index of numpy_vectors\n",
    "    \"\"\"\n",
    "\n",
    "    search_index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "    search_index.addDataPointBatch(numpy_vectors)\n",
    "    search_index.createIndex({'post': 2}, print_progress=True)\n",
    "    return search_index\n",
    "search_index = create_nmslib_search_index(embedings_eco)\n",
    "search_index.saveIndex('multilingual_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Tokenizer: raw_file_en.txt in language en  \n",
      " - fast BPE: processing tok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 82.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. how will monetary policy affect interest rates?  :::   0.16479647\n",
      "\n",
      "what is the effect of prices?  :::   0.18911254\n",
      "\n",
      "4. what is the impact of interest rate differentials on the exchange rate?  :::   0.1924169\n",
      "\n",
      "what exactly are floating exchange rates?  :::   0.21271539\n",
      "\n",
      "what is the impact of international currency exchange rate fluctuation?  :::   0.21645671\n",
      "\n",
      "why a low inflation rate target?  :::   0.2171886\n",
      "\n",
      "how would rising interest rates affect the banking sector?  :::   0.21922374\n",
      "\n",
      "what has been the experience with inflation targeting?  :::   0.21947384\n",
      "\n",
      "what implications did these policies have for inflation?  :::   0.22273636\n",
      "\n",
      "however what about the risk of inflation?  :::   0.22466302\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embed_demo = get_embeddings('en', 'raw_file_en.txt', ['how does inflation effect interest rate?'])\n",
    "\n",
    "ids, distances = search_index.knnQuery(embed_demo[0], k=10)\n",
    "\n",
    "for i, pos in enumerate(ids):\n",
    "    print(sentences[pos]+'  :::   '+str(distances[i])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Tokenizer: raw_file_en.txt in language nl  \n",
      " - fast BPE: processing tok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 131.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. how will monetary policy affect interest rates?  :::   0.19387078\n",
      "\n",
      "what is the effect of prices?  :::   0.19455987\n",
      "\n",
      "what has been the experience with inflation targeting?  :::   0.21006119\n",
      "\n",
      "what implications did these policies have for inflation?  :::   0.21268898\n",
      "\n",
      "what is the impact of international currency exchange rate fluctuation?  :::   0.22213888\n",
      "\n",
      "however what about the risk of inflation?  :::   0.22290683\n",
      "\n",
      "what are the implications for monetary policy?  :::   0.22872382\n",
      "\n",
      "what exactly are floating exchange rates?  :::   0.23408651\n",
      "\n",
      "but consider the effect of inflation.  :::   0.23626012\n",
      "\n",
      "4. what is the impact of interest rate differentials on the exchange rate?  :::   0.24321729\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embed_demo = get_embeddings('nl', 'raw_file_en.txt', ['hoe werkt inflatie-effect rente?'])\n",
    "\n",
    "ids, distances = search_index.knnQuery(embed_demo[0], k=10)\n",
    "\n",
    "for i, pos in enumerate(ids):\n",
    "    print(sentences[pos]+'  :::   '+str(distances[i])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Tokenizer: raw_file_en.txt in language gem  \n",
      " - fast BPE: processing tok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 98.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. how will monetary policy affect interest rates?  :::   0.16120678\n",
      "\n",
      "what is the effect of prices?  :::   0.19260627\n",
      "\n",
      "what implications did these policies have for inflation?  :::   0.20439976\n",
      "\n",
      "4. what is the impact of interest rate differentials on the exchange rate?  :::   0.2088108\n",
      "\n",
      "what would be the impact if any on the exchange rate?  :::   0.21565342\n",
      "\n",
      "what has been the experience with inflation targeting?  :::   0.21710718\n",
      "\n",
      "how would rising interest rates affect the banking sector?  :::   0.21808034\n",
      "\n",
      "however what about the risk of inflation?  :::   0.22759044\n",
      "\n",
      "what exactly are floating exchange rates?  :::   0.22852325\n",
      "\n",
      "what is the impact of international currency exchange rate fluctuation?  :::   0.23033577\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embed_demo = get_embeddings('gem', 'raw_file_en.txt', ['Wie wirkt sich die Inflation auf den Zinssatz aus?'])\n",
    "\n",
    "ids, distances = search_index.knnQuery(embed_demo[0], k=10)\n",
    "\n",
    "for i, pos in enumerate(ids):\n",
    "    print(sentences[pos]+'  :::   '+str(distances[i])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Tokenizer: raw_file_en.txt in language it  \n",
      " - fast BPE: processing tok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 119.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. how will monetary policy affect interest rates?  :::   0.14336467\n",
      "\n",
      "how would rising interest rates affect the banking sector?  :::   0.19409412\n",
      "\n",
      "what is the effect of prices?  :::   0.20226151\n",
      "\n",
      "4. what is the impact of interest rate differentials on the exchange rate?  :::   0.20520967\n",
      "\n",
      "1. how does infl ation affect nominal interest rates?  :::   0.20935893\n",
      "\n",
      "what is the impact of international currency exchange rate fluctuation?  :::   0.21694893\n",
      "\n",
      "what implications did these policies have for inflation?  :::   0.22093427\n",
      "\n",
      "what exactly are floating exchange rates?  :::   0.22280097\n",
      "\n",
      "what has been the experience with inflation targeting?  :::   0.22607595\n",
      "\n",
      "what would be the impact if any on the exchange rate?  :::   0.2266792\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embed_demo = get_embeddings('it', 'raw_file_en.txt', ['in che modo l\\'inflazione influenza il tasso di interesse?'])\n",
    "\n",
    "ids, distances = search_index.knnQuery(embed_demo[0], k=10)\n",
    "\n",
    "for i, pos in enumerate(ids):\n",
    "    print(sentences[pos]+'  :::   '+str(distances[i])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Tokenizer: raw_file_en.txt in language es  \n",
      " - fast BPE: processing tok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 114.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. how will monetary policy affect interest rates?  :::   0.16222537\n",
      "\n",
      "what implications did these policies have for inflation?  :::   0.21176767\n",
      "\n",
      "1. how does infl ation affect nominal interest rates?  :::   0.2151556\n",
      "\n",
      "how would rising interest rates affect the banking sector?  :::   0.21975899\n",
      "\n",
      "what has been the experience with inflation targeting?  :::   0.23229277\n",
      "\n",
      "4. what is the impact of interest rate differentials on the exchange rate?  :::   0.23261261\n",
      "\n",
      "already a function of the present price and the rate of interest?  :::   0.23437083\n",
      "\n",
      "what is the effect of prices?  :::   0.23609203\n",
      "\n",
      "however what about the risk of inflation?  :::   0.240129\n",
      "\n",
      "what exactly are floating exchange rates?  :::   0.24489737\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embed_demo = get_embeddings('es', 'raw_file_en.txt', ['¿Cómo afecta la inflación a la tasa de interés?'])\n",
    "\n",
    "ids, distances = search_index.knnQuery(embed_demo[0], k=10)\n",
    "\n",
    "for i, pos in enumerate(ids):\n",
    "    print(sentences[pos]+'  :::   '+str(distances[i])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Tokenizer: raw_file_en.txt in language hn  \n",
      " - fast BPE: processing tok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 59.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a straggly man in a dark hat and glasses stands in a corner brooding.  :::   0.28005415\n",
      "\n",
      "1 simply 5 中华人民共和国中国人民银行法 chineseenglish in cch asia pacific eds.  :::   0.28070813\n",
      "\n",
      "the slope and rsquared in the scatter plots are bigger in fig.  :::   0.28273642\n",
      "\n",
      "i use the edition shinko kenkyu yakuchu hen 眞誥研究 譯注篇 tadao yoshikawa 吉川忠夫 and kunio mugitani 麥谷 邦夫 eds.  :::   0.2835586\n",
      "\n",
      "model ln δrpit  ln δrft  αi  βi1ln δrmit  ln δrft  ɛit.  :::   0.28709984\n",
      "\n",
      "in selgins view the credit 56 moneysoundandunsoundsalerno inconclusionmodernaustrianmonetarythoughtwithits rootsintheturgottraditionandemphasisonthemacroeconomic phenomenaofentrepreneurialcalculationandpricecoodination standsinradicaloppositiontothemodernmacroeconomicschools ofthoughtwhosemonetarydoctrineshavebeenmoldedwithinthe lawtradition.  :::   0.287973\n",
      "\n",
      "model ln δrpit  ln δrft  αi  βi1ln δrmit  ln δrft  βi2smbt  βi3hmlt  βi4momt  ɛit.  :::   0.29057956\n",
      "\n",
      "httpwww.consumerfinance.govnewsroomcfpbtakesactionagainstacecashexpressforpushingpaydayborrowersintocycleofdebt httpwww.consumerfinance.govnewsroomcfpbtakesactionagainstacecashexpressforpushingpaydayborrowersintocycleofdebt www.consumerfinance.govnewsroomcfpbsuesforprofitcollegechainittforpredatorylending httpwww.consumerfinance.govnewsroomcfpbsuescashcallforillegalonlineloanservicing httpwww.consumerfinance.govnewsroomcfpbsuescashcallforillegalonlineloanservicing httpwww.consumerfinance.govnewsroomcfpbtakesactiontostopfloridacompanyfromengaginginillegaldebtreliefpractices httpwww.consumerfinance.govnewsroomcfpbtakesactiontostopfloridacompanyfromengaginginillegaldebtreliefpractices www.consumerfinance.govnewsroomcfpbsuesforprofitcollegechainittforpredatorylending 96 k.c.  :::   0.29171038\n",
      "\n",
      "model ln δrpit  ln δrft  αi  βi1ln δrmit  ln δrft  βi2smbt  βi3hmlt ɛit.  :::   0.2934013\n",
      "\n",
      "the drawee tmv mgmanjks or obavtto qtt or mtawsst by.  :::   0.29503047\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embed_demo = get_embeddings('hn', 'raw_file_en.txt', ['मुद्रास्फीति पर ब्याज दर कैसे प्रभाव डालती है?'])\n",
    "\n",
    "ids, distances = search_index.knnQuery(embed_demo[0], k=10)\n",
    "\n",
    "for i, pos in enumerate(ids):\n",
    "    print(sentences[pos]+'  :::   '+str(distances[i])+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
